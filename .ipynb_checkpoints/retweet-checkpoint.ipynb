{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442e9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a023c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweet_data.csv')\n",
    "print(\"number of users: \", df.username.unique().shape[0])\n",
    "print(\"columns\", df.columns)\n",
    "\n",
    "userlist_shuffled = pd.Series(df.username.unique()).sample(frac=1, random_state=26).reset_index(drop=True).tolist()\n",
    "\n",
    "mid = 4 * (len(userlist_shuffled) // 5)\n",
    "train = df[df.username.isin(userlist_shuffled[:mid])]\n",
    "test = df[df.username.isin(userlist_shuffled[mid:])]\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"train shape: \", train.shape, \"test shape\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5701caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retweet(tw):\n",
    "    if tw.startswith('RT @'): \n",
    "        return tw.split(\" \")[1][1:-1]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def concat_retweets(df, column):\n",
    "    \"\"\"\n",
    "    concatenates tweets for each user\n",
    "    returns dataframe including usernames and corresponding concatenated tweets\n",
    "    \"\"\"\n",
    "    map_user = list()\n",
    "    map_retweets = list()\n",
    "    \n",
    "    for user in tqdm(df.username.unique()):\n",
    "        map_user.append(user)\n",
    "        map_retweets.append(\" \".join([get_retweet(text) for text in df[df.username == user][column].values]))\n",
    "  \n",
    "    return pd.DataFrame({\"username\":map_user, \"retweet_list\":map_retweets}, columns=[\"username\", \"retweet_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc5ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_user_tweet_tr = concat_retweets(train, 'text')\n",
    "map_user_tweet_ts = concat_retweets(test, 'text')\n",
    "\n",
    "merged_tr = map_user_tweet_tr.merge(train[['username',\n",
    "                   'realname','meslek', 'age_group',\n",
    "                   'age_enc', 'gender', 'gender_enc']].drop_duplicates(), how=\"left\", left_on=\"username\", right_on=\"username\")\n",
    "\n",
    "merged_ts = map_user_tweet_ts.merge(test[['username',\n",
    "                   'realname','meslek', 'age_group',\n",
    "                   'age_enc', 'gender', 'gender_enc']].drop_duplicates(), how=\"left\", left_on=\"username\", right_on=\"username\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bef4401",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged_tr.append(merged_ts)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "N = 20\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), \n",
    "                     max_features=N,\n",
    "                     analyzer='word')\n",
    "vectorizer.fit(merged[\"retweet_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a1a218a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rt_csr = vectorizer.transform(merged_tr[\"retweet_list\"])\n",
    "test_rt_csr = vectorizer.transform(merged_ts[\"retweet_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "85024920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "models = [SVC(), RandomForestClassifier()]\n",
    "labels = [\"age_enc\", \"gender_enc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c529e5f6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d1a4a962e24794bbe91756b3fecaca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_enc macro SVC f1: 0.227, acc: 0.342, pre: 0.257, rec: 0.321\n",
      "0    73\n",
      "3    38\n",
      "2    36\n",
      "4     2\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9db609d74b4737b210cdf4106fc93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender_enc binary SVC f1: 0.700, acc: 0.564, pre: 0.905, rec: 0.571\n",
      "0    133\n",
      "1     16\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dae0d337aae46be91e3be368f647fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_enc macro RandomForestClassifier f1: 0.294, acc: 0.423, pre: 0.317, rec: 0.332\n",
      "0    74\n",
      "2    30\n",
      "3    29\n",
      "1     8\n",
      "4     8\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076fc2fd59364d4c81db4e4a6de45f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender_enc binary RandomForestClassifier f1: 0.718, acc: 0.611, pre: 0.881, rec: 0.607\n",
      "0    122\n",
      "1     27\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for model_to_fit in models:\n",
    "    for label in labels:\n",
    "        # fit model\n",
    "        model = model_to_fit\n",
    "        model.fit(train_rt_csr, merged_tr[label])\n",
    "        \n",
    "        preds=list()\n",
    "        for test_vec_csr in tqdm(test_rt_csr):\n",
    "            pred = model.predict(test_vec_csr)[0]\n",
    "            preds.append(pred)\n",
    "        \n",
    "        if merged_ts[label].nunique() > 2:\n",
    "            print(label, 'macro', type(model).__name__, \"f1:\", \"{:.3f},\".format(f1_score(preds, merged_ts[label], average='macro')),\n",
    "                \"acc:\", \"{:.3f},\".format(accuracy_score(preds, merged_ts[label])),\n",
    "                 \"pre:\", \"{:.3f},\".format(precision_score(preds, merged_ts[label], average='macro')),\n",
    "                 \"rec:\", \"{:.3f}\".format(recall_score(preds, merged_ts[label], average='macro')))\n",
    "            print(pd.Series(preds).value_counts())\n",
    "        else:\n",
    "            print(label, 'binary', type(model).__name__, \"f1:\", \"{:.3f},\".format(f1_score(preds, merged_ts[label], average='binary', pos_label=0)),\n",
    "                \"acc:\", \"{:.3f},\".format(accuracy_score(preds, merged_ts[label])),\n",
    "                 \"pre:\", \"{:.3f},\".format(precision_score(preds, merged_ts[label], average='binary', pos_label=0)),\n",
    "                 \"rec:\", \"{:.3f}\".format(recall_score(preds, merged_ts[label], average='binary', pos_label=0)))\n",
    "            print(pd.Series(preds).value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
