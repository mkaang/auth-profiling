{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b310e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2328e731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users:  729\n",
      "columns Index(['id_str', 'created_at', 'text', 'entities', 'retweeted', 'username',\n",
      "       'realname', 'gender', 'age', 'meslek', 'gender_enc', 'age_group',\n",
      "       'age_enc'],\n",
      "      dtype='object')\n",
      "train shape:  (1094507, 13) test shape (275204, 13)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('tweet_data.csv')\n",
    "print(\"number of users: \", df.username.unique().shape[0])\n",
    "print(\"columns\", df.columns)\n",
    "\n",
    "userlist_shuffled = pd.Series(df.username.unique()).sample(frac=1, random_state=26).reset_index(drop=True).tolist()\n",
    "\n",
    "mid = 4 * (len(userlist_shuffled) // 5)\n",
    "train = df[df.username.isin(userlist_shuffled[:mid])]\n",
    "test = df[df.username.isin(userlist_shuffled[mid:])]\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"train shape: \", train.shape, \"test shape\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd870ffc",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f0a6924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Bozoklularr: Herkesin 羹niversitesi a癟覺lm覺, bir kafelerde,sokaklarda story at覺yorlar.\n",
      "Allah'覺m nasip et.仆\n",
      "RT @Bozoklularr: Herkesin 羹niversitesi a癟覺lm覺, bir kafelerde,sokaklarda story at覺yorlar.\n",
      "Allah'覺m nasip et.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "text = df.text[0]\n",
    "print(text) # with emoji\n",
    "\n",
    "print(remove_emoji(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a1fc4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(entities_df, text_df):\n",
    "    if isinstance(text_df, list): \n",
    "        prep_list = list()\n",
    "        for ent_df, txt_df in tqdm(zip(entities_df, text_df)):\n",
    "            prep_text=preprocess_ind(ent_df, txt_df)\n",
    "            prep_list.append(prep_text)\n",
    "        return prep_list\n",
    "    else:\n",
    "        return preprocess_ind(entities_df, text_df)\n",
    "    \n",
    "def preprocess_ind(entities_df, text_df):\n",
    "    ent_dict = eval(entities_df)\n",
    "    text = text_df\n",
    "    \n",
    "    # Extract entities directly from tweet entities\n",
    "    texts_to_extract = []\n",
    "    for key in ent_dict.keys():\n",
    "        if ent_dict[key]:\n",
    "            for ent_dict_2 in ent_dict[key]:\n",
    "                inds = ent_dict_2['indices']\n",
    "                texts_to_extract.append(text[inds[0]:inds[1]])\n",
    "    for text_to_extract in texts_to_extract:\n",
    "        text = text.replace(text_to_extract, '')\n",
    "        \n",
    "    # extract any words starting with # or @ or links starting with https\n",
    "    for tag in text.split(): \n",
    "        if tag.startswith(\"#\") or tag.startswith(\"@\") or tag.startswith(\"https://\"):\n",
    "            text=text.replace(tag, '')\n",
    "    \n",
    "    # extract newline and RT\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('RT : ', '')\n",
    "    \n",
    "    #demojidfy\n",
    "    text = remove_emoji(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a930fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Herkesin 羹niversitesi a癟覺lm覺, bir kafelerde,sokaklarda story at覺yorlar. Allah'覺m nasip et.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(df.entities[0], df.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1fa71c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9898b689188b4305b0413d9814c74677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-710db128d8ff>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"processed_text\"] = preprocess(train.entities.tolist(), train.text.tolist())\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bed672cc9374f719ebe9459b7efcec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-710db128d8ff>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"processed_text\"] = preprocess(test.entities.tolist(), test.text.tolist())\n"
     ]
    }
   ],
   "source": [
    "train[\"processed_text\"] = preprocess(train.entities.tolist(), train.text.tolist())\n",
    "test[\"processed_text\"] = preprocess(test.entities.tolist(), test.text.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bb4a28",
   "metadata": {},
   "source": [
    "#### stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50a9cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "stemmer=SnowballStemmer('porter')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def stem(text_df, stemmer):\n",
    "    stemmed_list = []\n",
    "    if isinstance(text_df, list): \n",
    "        for text in tqdm(text_df):\n",
    "            stemmed_list.append(stem_ind(text, stemmer))\n",
    "        return stemmed_list\n",
    "    else:\n",
    "        return stem_ind(text_df, stemmer)\n",
    "        \n",
    "def stem_ind(text_df, stemmer):\n",
    "    # Stem each word\n",
    "    stemmed =\"\"\n",
    "    #import pdb; pdb.set_trace()\n",
    "    worded = tokenizer.tokenize(text_df)\n",
    "    for word in worded:\n",
    "        stemmed = stemmed + stemmer.stem(word) + ' '\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb8f5202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564122d655414b7aa77c5e99eb8fef65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1094507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-8b59ef1a1e80>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"stemmed\"] = stem(train[\"processed_text\"].tolist(), stemmer)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f8f1ad92fc493dbe0bc078871d8300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/275204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-8b59ef1a1e80>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"stemmed\"] = stem(test[\"processed_text\"].tolist(), stemmer)\n"
     ]
    }
   ],
   "source": [
    "train[\"stemmed\"] = stem(train[\"processed_text\"].tolist(), stemmer)\n",
    "test[\"stemmed\"] = stem(test[\"processed_text\"].tolist(), stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "556930e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_tweets(df, column):\n",
    "    \"\"\"\n",
    "    concatenates tweets for each user\n",
    "    returns dataframe including usernames and corresponding concatenated tweets\n",
    "    \"\"\"\n",
    "    map_user = list()\n",
    "    map_tweets = list()\n",
    "    \n",
    "    for user in tqdm(df.username.unique()):\n",
    "        map_user.append(user)\n",
    "        tweets_concat = \" \".join(df[df.username == user][column].values)\n",
    "        map_tweets.append(tweets_concat)\n",
    "        \n",
    "    return pd.DataFrame({\"username\":map_user, f\"tw_concat_{column}\":map_tweets}, columns=[\"username\", f\"tw_concat_{column}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1f1c8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d67e5afd0684fccbec794025ea87781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834248ba74f741f6986e08c276153da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11c74dbde484915bf87a78a84676a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3c999b1f084814be99144c8f41bc3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "map_user_tweet_tr_stem = concat_tweets(train, 'stemmed')\n",
    "map_user_tweet_ts_stem = concat_tweets(test, 'stemmed')\n",
    "\n",
    "map_user_tweet_tr_process = concat_tweets(train, 'processed_text')\n",
    "map_user_tweet_ts_process = concat_tweets(test, 'processed_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05621480",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_user_tweet_tr = map_user_tweet_tr_stem.merge(map_user_tweet_tr_process, on=\"username\")\n",
    "map_user_tweet_ts = map_user_tweet_ts_stem.merge(map_user_tweet_ts_process, on=\"username\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21fa52ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This cell just to be run for specifying years\n",
    "train['year'] = train.created_at.apply(lambda x: x[:4])\n",
    "test['year'] = test.created_at.apply(lambda x: x[:4])\n",
    "\n",
    "merged_tr = map_user_tweet_tr.merge(train[['username',\n",
    "                   'realname','meslek', 'age_group',\n",
    "                   'age_enc', 'gender', 'gender_enc']].drop_duplicates(), how=\"left\", left_on=\"username\", right_on=\"username\")\n",
    "\n",
    "merged_ts = map_user_tweet_ts.merge(test[['username',\n",
    "                   'realname','meslek', 'age_group',\n",
    "                   'age_enc', 'gender', 'gender_enc']].drop_duplicates(), how=\"left\", left_on=\"username\", right_on=\"username\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5019daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tr.to_csv(\"user_tweets_train.csv\", index=False)\n",
    "merged_ts.to_csv(\"user_tweets_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee851132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# train_2021 = train[train.year == \"2021\"]\n",
    "# test_2021 = test[test.year == \"2021\"]\n",
    "\n",
    "# map_user_tweet_tr = concat_tweets(train_2021, 'stemmed')\n",
    "# map_user_tweet_ts = concat_tweets(test_2021, 'stemmed')\n",
    "\n",
    "# merged_tr = map_user_tweet_tr.merge(train_2021[['username',\n",
    "#                    'realname','meslek', 'age_group',\n",
    "#                    'age_enc', 'gender', 'gender_enc']].drop_duplicates(), how=\"left\", left_on=\"username\", right_on=\"username\")\n",
    "\n",
    "# merged_ts = map_user_tweet_ts.merge(test_2021[['username',\n",
    "#                    'realname','meslek', 'age_group',\n",
    "#                    'age_enc', 'gender', 'gender_enc']].drop_duplicates(), how=\"left\", left_on=\"username\", right_on=\"username\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec7054",
   "metadata": {},
   "source": [
    "### Prepare Data for BERTTurk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd944c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_user_tweet_tr = concat_tweets(train, 'processed_text')\n",
    "# map_user_tweet_ts = concat_tweets(test, 'processed_text')\n",
    "\n",
    "# merged_tr = map_user_tweet_tr.merge(train[['username',\n",
    "#                    'realname','meslek', 'age_group',\n",
    "#                    'age_enc', 'gender', 'gender_enc']].drop_duplicates(), how=\"left\", left_on=\"username\", right_on=\"username\")\n",
    "\n",
    "# merged_ts = map_user_tweet_ts.merge(test[['username',\n",
    "#                    'realname','meslek', 'age_group',\n",
    "#                    'age_enc', 'gender', 'gender_enc']].drop_duplicates(), how=\"left\", left_on=\"username\", right_on=\"username\")\n",
    "\n",
    "# def process_data(df, label):\n",
    "#     column_titles = ['sentence1', 'label']\n",
    "#     df = df.rename(columns={'tw_concat':'sentence1', label:'label'})\n",
    "#     df = df[column_titles]\n",
    "#     df = df.reindex(columns=column_titles)\n",
    "#     return df\n",
    "\n",
    "# process_data(merged_tr, 'gender_enc').to_csv('train_gender.csv',index=False)\n",
    "# process_data(merged_ts, 'gender_enc').to_csv('test_gender.csv',index=False)\n",
    "\n",
    "# process_data(merged_tr, 'age_enc').to_csv('train_age.csv',index=False)\n",
    "# process_data(merged_ts, 'age_enc').to_csv('test_age.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
