{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "180661ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a1a5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_transform(serie, vectorizer, chunk_size = 10_000):\n",
    "    number_chunks = len(serie) // chunk_size\n",
    "    tf_idf_matrix = scipy.sparse.csr.csr_matrix([])\n",
    "    for c, chunk in tqdm(enumerate(np.array_split(serie, number_chunks))):\n",
    "        if not c:\n",
    "            tf_idf_matrix = vectorizer.transform(chunk)\n",
    "        else:\n",
    "            tf_idf_matrix_part = vectorizer.transform(chunk)\n",
    "            tf_idf_matrix = scipy.sparse.vstack((tf_idf_matrix, tf_idf_matrix_part))\n",
    "\n",
    "    return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4a36e",
   "metadata": {},
   "source": [
    "### Filtered by years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afcc740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_tweets(df, column):\n",
    "    \"\"\"\n",
    "    concatenates tweets for each user\n",
    "    returns dataframe including usernames and corresponding concatenated tweets\n",
    "    \"\"\"\n",
    "    map_user = list()\n",
    "    map_tweets = list()\n",
    "    \n",
    "    for user in tqdm(df.username.unique()):\n",
    "        map_user.append(user)\n",
    "        tweets_concat = \" \".join(df[df.username == user][column].values)\n",
    "        map_tweets.append(tweets_concat)\n",
    "        \n",
    "    return pd.DataFrame({\"username\":map_user, \"tw_concat\":map_tweets}, columns=[\"username\", \"tw_concat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3403e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"tweets_train.csv\", low_memory=False)\n",
    "test=pd.read_csv(\"tweets_test.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c7fa59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(subset=[\"processed_text\"], inplace=True)\n",
    "test.dropna(subset=[\"processed_text\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0529d716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8710fa8891e84c3fbfb872ffbeb4b60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/541 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915febf6da424b089d8408f435b9fe83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b7ec9fdc50482b9502996ca6ac7ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce99caa8f6db4e21b849a15f80831272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf8503104354e46ad31d0cf74c19775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8193aab8bffa4d3085d592a4766252cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0444453710fe41d9ac54c75be9eff4b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ee5ad00e8946d0ba4bc9d8db3a79b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5012269b4d6407e98ef945300953c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef98d433fe914ada8c9c593f3e8e8dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618be179d9eb4136ba933187136e88a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5038c04967414dfea02be6ad7d03c361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3f5521ec0c45fda3b1175b17ab9c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae48f26b9e44b0496d28c523bdf6e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02c50c6dcde4fba882b13eb290525bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ebe51f43cda4aacb300e1a34d348b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b7f22ba49a4d8f943e0c049a78ccc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617866edd83d4a96919bfd078e420ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2ba97584be4238bf0099e704c98e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee99478d96834d6f9dd962df6d21f081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde97f2529244dbabff29440776e14d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/557 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed07d59ed9e498ab268ee4f332575c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383d4e01df32468a925bee9b715c669a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4ae4cc153f4af2b90cdacb1fe2935a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3393004f5b458eb4eee6be3e978cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca3dc941a8a4c9f9e19bb2ead4e3f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656a196a3dca4b62af0f562ae92736e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584982f925594b3e9d80fde6a89126a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff22bb83fbc4d69896c6c6e086efceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff36f7c34a249cb8483506e53cb8f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "YEARS = [2020, 2019, 2018]\n",
    "\n",
    "results = []\n",
    "\n",
    "for YEAR in YEARS:\n",
    "    \n",
    "    print(YEAR)\n",
    "    train_year = train[train.year.isin(range(YEAR,2022))]\n",
    "    test_year = test[test.year.isin(range(YEAR,2022))]\n",
    "\n",
    "    map_user_tweet_tr = concat_tweets(train_year, 'processed_text')\n",
    "    map_user_tweet_ts = concat_tweets(test_year, 'processed_text')\n",
    "\n",
    "    merged_tr = map_user_tweet_tr.merge(train[['username',\n",
    "                       'realname','meslek', 'age_group',\n",
    "                       'age_enc', 'gender', 'gender_enc']].drop_duplicates(subset=['username']), how=\"left\", left_on=\"username\", right_on=\"username\")\n",
    "\n",
    "    merged_ts = map_user_tweet_ts.merge(test[['username',\n",
    "                       'realname','meslek', 'age_group',\n",
    "                       'age_enc', 'gender', 'gender_enc']].drop_duplicates(subset=['username']), how=\"left\", left_on=\"username\", right_on=\"username\")\n",
    "\n",
    "    \n",
    "    for max_features in [5_000, 10_000]:\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1,2), \n",
    "                             max_features=max_features,\n",
    "                             analyzer='word')\n",
    "\n",
    "        fitted_vectorizer = vectorizer.fit(merged_tr.tw_concat)\n",
    "\n",
    "        train_vecs_csr = progress_transform(merged_tr.tw_concat, fitted_vectorizer, chunk_size = 100)\n",
    "        test_vecs_csr = progress_transform(merged_ts.tw_concat, fitted_vectorizer, chunk_size = 100)\n",
    "\n",
    "\n",
    "        labels = [\"gender_enc\"] # age_enc\n",
    "        BIAS = False\n",
    "\n",
    "        for label in labels:\n",
    "\n",
    "            models = [SVC(), RandomForestClassifier()]\n",
    "\n",
    "            if BIAS:\n",
    "                classes = merged_tr[label].unique().tolist()\n",
    "                classWeight = compute_class_weight('balanced', classes, merged_tr[label]) \n",
    "                classWeight = dict(enumerate(classWeight))\n",
    "                models = [SVC(class_weight=classWeight), RandomForestClassifier(class_weight=classWeight)]\n",
    "\n",
    "            for model_to_fit in models:\n",
    "                # fit model\n",
    "                model = model_to_fit\n",
    "                model.fit(train_vecs_csr, merged_tr[label])\n",
    "\n",
    "                preds=list()\n",
    "                for test_vec_csr in tqdm(test_vecs_csr):\n",
    "                    pred = model.predict(test_vec_csr)[0]\n",
    "                    preds.append(pred)\n",
    "\n",
    "                average = None\n",
    "                if merged_ts[label].nunique() > 2:\n",
    "                    average = 'macro'\n",
    "                else:\n",
    "                    average = 'binary'\n",
    "                \n",
    "                f1 = \"{:.3f}\".format(f1_score(preds, merged_ts[label], average=average))\n",
    "                acc = \"{:.3f}\".format(accuracy_score(preds, merged_ts[label]))\n",
    "                pre = \"{:.3f}\".format(precision_score(preds, merged_ts[label], average=average))\n",
    "                rec = \"{:.3f}\".format(recall_score(preds, merged_ts[label], average=average))\n",
    "                \n",
    "                results.append([f1, acc, pre, rec])\n",
    "                \n",
    "#                 print(label, type(model).__name__, \"f1:\", \"{:.3f},\".format(f1_score(preds, merged_ts[label], average=average)),\n",
    "#                                                   \"acc:\", \"{:.3f},\".format(accuracy_score(preds, merged_ts[label])),\n",
    "#                                                   \"pre:\", \"{:.3f},\".format(precision_score(preds, merged_ts[label], average=average)),\n",
    "#                                                   \"rec:\", \"{:.3f}\".format(recall_score(preds, merged_ts[label], average=average)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c03391f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>acc</th>\n",
       "      <th>pre</th>\n",
       "      <th>rec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.586</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.615</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.592</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.580</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.588</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.661</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.588</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.667</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.629</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.654</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.583</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       f1    acc    pre    rec\n",
       "0   0.586  0.707  0.475  0.763\n",
       "1   0.615  0.714  0.525  0.744\n",
       "2   0.592  0.714  0.475  0.784\n",
       "3   0.580  0.700  0.475  0.744\n",
       "4   0.588  0.710  0.484  0.750\n",
       "5   0.661  0.738  0.597  0.740\n",
       "6   0.588  0.710  0.484  0.750\n",
       "7   0.667  0.752  0.581  0.783\n",
       "8   0.629  0.733  0.532  0.767\n",
       "9   0.654  0.753  0.548  0.810\n",
       "10  0.583  0.705  0.484  0.732\n",
       "11  0.600  0.699  0.532  0.688"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results, columns=[\"f1\",\"acc\",\"pre\",\"rec\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0099cd50",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d610b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tr=pd.read_csv(\"data/user_tweets_train.csv\")\n",
    "merged_ts=pd.read_csv(\"data/user_tweets_test.csv\")\n",
    "# print(merged_tr.shape, merged_ts.shape)\n",
    "\n",
    "dup_usernames = np.load('duplicated_usernames.npy', allow_pickle=True)\n",
    "\n",
    "merged_tr.dropna(subset=[\"tw_concat_stemmed\"], inplace=True)\n",
    "merged_tr = merged_tr[~merged_tr.username.isin(dup_usernames)]\n",
    "\n",
    "merged_ts.dropna(subset=[\"tw_concat_stemmed\"], inplace=True)\n",
    "merged_ts = merged_ts[~merged_ts.username.isin(dup_usernames)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df7cbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(833, 9) (207, 9)\n"
     ]
    }
   ],
   "source": [
    "print(merged_tr.shape, merged_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89c4fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mid = (merged_tr.shape[0] // 4) * 3\n",
    "merged_ts = merged_tr[mid:]\n",
    "merged_tr = merged_tr[:mid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e411f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(624, 9) (209, 9)\n"
     ]
    }
   ],
   "source": [
    "print(merged_tr.shape, merged_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cbeba94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    110\n",
       "1     99\n",
       "Name: gender_enc, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_ts.gender_enc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa4ca82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2652/2581104456.py:3: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  tf_idf_matrix = scipy.sparse.csr.csr_matrix([])\n",
      "6it [00:15,  2.62s/it]\n",
      "2it [00:05,  2.51s/it]\n",
      "/tmp/ipykernel_2652/2581104456.py:3: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  tf_idf_matrix = scipy.sparse.csr.csr_matrix([])\n",
      "6it [00:14,  2.41s/it]\n",
      "2it [00:04,  2.32s/it]\n",
      "/tmp/ipykernel_2652/2581104456.py:3: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  tf_idf_matrix = scipy.sparse.csr.csr_matrix([])\n",
      "6it [00:15,  2.66s/it]\n",
      "2it [00:05,  2.58s/it]\n"
     ]
    }
   ],
   "source": [
    "BIAS = False\n",
    "results = []\n",
    "\n",
    "models = [SVC(), RandomForestClassifier(), LogisticRegression(), KNeighborsClassifier()]\n",
    "models = [LogisticRegression()]\n",
    "\n",
    "labels = [\"age_enc\", \"gender_enc\"]\n",
    "labels = [\"gender_enc\"]\n",
    "\n",
    "preds_all = []\n",
    "\n",
    "for max_features in [5_000, 10_000, 20_000]:\n",
    "\n",
    "    for label in labels:\n",
    "        \n",
    "        # if label == 'age_enc':\n",
    "        #     max_features = 20_000\n",
    "        # else:\n",
    "        #     max_features = 5_000\n",
    "\n",
    "        # vectorizer = TfidfVectorizer(ngram_range=(3,3), \n",
    "        #                     max_features=max_features,\n",
    "        #                     analyzer='char_wb')\n",
    "\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1,2), \n",
    "                            max_features=max_features,\n",
    "                            analyzer='word')\n",
    "\n",
    "\n",
    "        fitted_vectorizer = vectorizer.fit(merged_tr.tw_concat_stemmed) # tw_concat_processed_text tw_concat_stemmed\n",
    "\n",
    "        train_vecs_csr = progress_transform(merged_tr.tw_concat_stemmed, fitted_vectorizer, chunk_size = 100)\n",
    "        test_vecs_csr = progress_transform(merged_ts.tw_concat_stemmed, fitted_vectorizer, chunk_size = 100)\n",
    "\n",
    "        for model_to_fit in models:\n",
    "            \n",
    "            if type(model_to_fit).__name__ == 'RandomForestClassifier':\n",
    "                num_runs = 3\n",
    "            else:\n",
    "                num_runs = 1\n",
    "\n",
    "            for _ in range(num_runs):\n",
    "\n",
    "                if BIAS:\n",
    "                    classes = merged_tr[label].unique().tolist()\n",
    "                    classWeight = compute_class_weight('balanced', classes, merged_tr[label]) \n",
    "                    classWeight = dict(enumerate(classWeight))\n",
    "                    models = [SVC(class_weight=classWeight), RandomForestClassifier(class_weight=classWeight)]\n",
    "\n",
    "                # fit model\n",
    "                model = model_to_fit\n",
    "                model.fit(train_vecs_csr, merged_tr[label])\n",
    "\n",
    "                preds=list()\n",
    "                for test_vec_csr in test_vecs_csr:\n",
    "                    pred = model.predict(test_vec_csr)[0]\n",
    "                    preds.append(pred)\n",
    "\n",
    "                preds_all.append(preds)\n",
    "\n",
    "                acc = \"{:.3f}\".format(accuracy_score(preds, merged_ts[label]))\n",
    "                \n",
    "                f1_macro = \"{:.3f}\".format(f1_score(preds, merged_ts[label], average='macro'))\n",
    "                pre_macro = \"{:.3f}\".format(precision_score(preds, merged_ts[label], average='macro'))\n",
    "                rec_macro = \"{:.3f}\".format(recall_score(preds, merged_ts[label], average='macro'))\n",
    "\n",
    "                f1_weighted = \"{:.3f}\".format(f1_score(preds, merged_ts[label], average='weighted'))\n",
    "                pre_weighted = \"{:.3f}\".format(precision_score(preds, merged_ts[label], average='weighted'))\n",
    "                rec_weighted = \"{:.3f}\".format(recall_score(preds, merged_ts[label], average='weighted'))\n",
    "\n",
    "                # f1_female = \"{:.3f}\".format(f1_score(preds, merged_ts[label], average='binary', pos_label=1))\n",
    "                # pre_female = \"{:.3f}\".format(precision_score(preds, merged_ts[label], average='binary',  pos_label=1))\n",
    "                # rec_female = \"{:.3f}\".format(recall_score(preds, merged_ts[label], average='binary',  pos_label=1))\n",
    "                \n",
    "                results.append([type(model_to_fit).__name__, acc, f1_macro, pre_macro, rec_macro, f1_weighted ])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "283c7f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.747 0.712 0.712 0.747\n"
     ]
    }
   ],
   "source": [
    "# print(\"{:.3f}\".format(precision_score(preds_all[2], merged_ts[label], average='binary', pos_label=1)),\n",
    "# \"{:.3f}\".format(precision_score(merged_ts[label], preds_all[2],  average='binary', pos_label=1)),\n",
    "# \"{:.3f}\".format(recall_score(preds_all[2], merged_ts[label],  average='binary', pos_label=1)),\n",
    "# \"{:.3f}\".format(recall_score(merged_ts[label], preds_all[2],  average='binary', pos_label=1)))\n",
    "\n",
    "# confusion_matrix(merged_ts[label], preds_all[2], labels=[1,0])\n",
    "\n",
    "# print(\"{:.3f}\".format(precision_score(preds_all[2], merged_ts[label], average='binary', pos_label=0)),\n",
    "# \"{:.3f}\".format(precision_score(merged_ts[label], preds_all[2],  average='binary', pos_label=0)),\n",
    "# \"{:.3f}\".format(recall_score(preds_all[2], merged_ts[label],  average='binary', pos_label=0)),\n",
    "# \"{:.3f}\".format(recall_score(merged_ts[label], preds_all[2],  average='binary', pos_label=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c065af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results, columns=[\"model\", \"acc\", 'f1_macro', 'pre_macro', 'rec_macro', 'f1_weighted'])#.to_csv('trash.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec090f",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45fdef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tr=pd.read_csv(\"data/user_tweets_train.csv\")\n",
    "merged_ts=pd.read_csv(\"data/user_tweets_test.csv\")\n",
    "\n",
    "dup_usernames = np.load('duplicated_usernames.npy', allow_pickle=True)\n",
    "\n",
    "merged_tr.dropna(subset=[\"tw_concat_stemmed\"], inplace=True)\n",
    "merged_tr = merged_tr[~merged_tr.username.isin(dup_usernames)]\n",
    "\n",
    "merged_ts.dropna(subset=[\"tw_concat_stemmed\"], inplace=True)\n",
    "merged_ts = merged_ts[~merged_ts.username.isin(dup_usernames)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a135d5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['username', 'tw_concat_stemmed', 'age_group',\n",
       "       'tw_concat_processed_text', 'realname', 'meslek', 'age_enc', 'gender',\n",
       "       'gender_enc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_tr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df9fb812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:45,  5.65s/it]\n",
      "2it [00:11,  5.52s/it]\n",
      "/home/kaan/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "8it [00:56,  7.05s/it]\n",
      "2it [00:12,  6.42s/it]\n"
     ]
    }
   ],
   "source": [
    "BIAS = False\n",
    "results = []\n",
    "\n",
    "models = [SVC(), RandomForestClassifier(), LogisticRegression(), KNeighborsClassifier()]\n",
    "labels = [\"age_enc\", \"gender_enc\"] \n",
    "\n",
    "for label in labels:\n",
    "    \n",
    "    if label == 'age_enc':\n",
    "        max_features = 20_000\n",
    "    else:\n",
    "        max_features = 5_000\n",
    "\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(3,3), \n",
    "                        max_features=max_features,\n",
    "                        analyzer='char_wb')\n",
    "\n",
    "    # vectorizer = TfidfVectorizer(ngram_range=(1,2), \n",
    "    #                     max_features=max_features,\n",
    "    #                     analyzer='word')\n",
    "\n",
    "\n",
    "    fitted_vectorizer = vectorizer.fit(merged_tr.tw_concat_processed_text)\n",
    "\n",
    "    train_vecs_csr = progress_transform(merged_tr.tw_concat_processed_text, fitted_vectorizer, chunk_size = 100)\n",
    "    test_vecs_csr = progress_transform(merged_ts.tw_concat_processed_text, fitted_vectorizer, chunk_size = 100)\n",
    "\n",
    "    for model_to_fit in [SVC(), KNeighborsClassifier()]:\n",
    "        \n",
    "        if type(model_to_fit).__name__ == 'RandomForestClassifier':\n",
    "            num_runs = 3\n",
    "        else:\n",
    "            num_runs = 1\n",
    "\n",
    "        for _ in range(num_runs):\n",
    "\n",
    "            if BIAS:\n",
    "                classes = merged_tr[label].unique().tolist()\n",
    "                classWeight = compute_class_weight('balanced', classes, merged_tr[label]) \n",
    "                classWeight = dict(enumerate(classWeight))\n",
    "                models = [SVC(class_weight=classWeight), RandomForestClassifier(class_weight=classWeight)]\n",
    "\n",
    "            # fit model\n",
    "            model = model_to_fit\n",
    "            model.fit(train_vecs_csr, merged_tr[label])\n",
    "\n",
    "            preds=list()\n",
    "            for test_vec_csr in test_vecs_csr:\n",
    "                pred = model.predict(test_vec_csr)[0]\n",
    "                preds.append(pred)\n",
    "\n",
    "            average = None\n",
    "            if merged_ts[label].nunique() > 2:\n",
    "                average = 'macro'\n",
    "            else:\n",
    "                average = 'binary'\n",
    "            \n",
    "            f1 = \"{:.3f}\".format(f1_score(preds, merged_ts[label], average=average))\n",
    "            acc = \"{:.3f}\".format(accuracy_score(preds, merged_ts[label]))\n",
    "            pre = \"{:.3f}\".format(precision_score(preds, merged_ts[label], average=average))\n",
    "            rec = \"{:.3f}\".format(recall_score(preds, merged_ts[label], average=average))\n",
    "            \n",
    "            results.append([type(model_to_fit).__name__, f1, acc, pre, rec])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5404831c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <th>f1</th>\n",
       "      <th>acc</th>\n",
       "      <th>pre</th>\n",
       "      <th>rec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   KNeighborsClassifier     f1    acc    pre    rec\n",
       "0                   SVC  0.366  0.498  0.390  0.374\n",
       "1  KNeighborsClassifier  0.397  0.483  0.400  0.432\n",
       "2                   SVC  0.711  0.749  0.727  0.696\n",
       "3  KNeighborsClassifier  0.658  0.633  0.830  0.545"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results, columns=[type(model_to_fit).__name__,\"f1\",\"acc\",\"pre\",\"rec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcd42e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7304347826086957"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(preds, merged_ts[label], average='binary', pos_label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cd00f1",
   "metadata": {},
   "source": [
    "0\tSVC\t                    0.366\t0.498\t0.390\t0.374\n",
    "1\tKNeighborsClassifier\t0.397\t0.483\t0.400\t0.432\n",
    "2\tSVC\t                    0.711\t0.749\t0.727\t0.696\n",
    "3\tKNeighborsClassifier\t0.658\t0.633\t0.830\t0.545"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104464d8",
   "metadata": {},
   "source": [
    "### Best Model Performance SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7af5060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_tr=pd.read_csv(\"turkish_dataset/dataset/train_prep.csv\").dropna(subset=[\"tw_concat_stemmed\"])\n",
    "# merged_ts=pd.read_csv(\"turkish_dataset/dataset/test_prep.csv\").dropna(subset=[\"tw_concat_stemmed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c495754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_ids = pd.read_csv(\"data/celeb_filtered.csv\", usecols=[\"user_id\"])\n",
    "celeb_info = pd.read_json(f'celebrity_profiling/ACL-19/celebrity-profiling/webis-celebrity-corpus-2019-distribution.ndjson', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ed3ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list()\n",
    "for label in celeb_info.labels:\n",
    "    try:\n",
    "        labels.append(label['sex or gender (P21)'].split()[0])\n",
    "    except:\n",
    "        labels.append(float(\"nan\"))\n",
    "\n",
    "celeb_info[\"gender\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "300f9571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male           46635\n",
       "female         18315\n",
       "transgender       40\n",
       "non-binary        18\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celeb_info[celeb_info[\"gender\"].isin([\"female\", \"male\"])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec968e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:04,  6.75it/s]\n",
      "19it [00:03,  6.33it/s]\n",
      "1924it [00:44, 43.29it/s]\n"
     ]
    }
   ],
   "source": [
    "label = \"label_x\"\n",
    "model = SVC(probability=True)\n",
    "max_features = 10_000\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), \n",
    "                    max_features=max_features,\n",
    "                    analyzer='word')\n",
    "\n",
    "fitted_vectorizer = vectorizer.fit(merged_tr.tw_concat_stemmed)\n",
    "\n",
    "train_vecs_csr = progress_transform(merged_tr.tw_concat_stemmed, fitted_vectorizer, chunk_size = 100)\n",
    "test_vecs_csr = progress_transform(merged_ts.tw_concat_stemmed, fitted_vectorizer, chunk_size = 100)\n",
    "\n",
    "model.fit(train_vecs_csr, merged_tr[label])\n",
    "\n",
    "preds=list()\n",
    "for test_vec_csr in tqdm(test_vecs_csr):\n",
    "    pred = model.predict(test_vec_csr)[0]\n",
    "    preds.append(pred)\n",
    "\n",
    "average = None\n",
    "if merged_ts[label].nunique() > 2:\n",
    "    average = 'macro'\n",
    "else:\n",
    "    average = 'binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10eb4a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.781', '0.800', '0.784', '0.778']\n"
     ]
    }
   ],
   "source": [
    "f1 = \"{:.3f}\".format(f1_score(preds, merged_ts[label], average=average, pos_label='male'))\n",
    "acc = \"{:.3f}\".format(accuracy_score(preds, merged_ts[label]))\n",
    "pre = \"{:.3f}\".format(precision_score(preds, merged_ts[label], average=average, pos_label='male'))\n",
    "rec = \"{:.3f}\".format(recall_score(preds, merged_ts[label], average=average, pos_label='male'))\n",
    "\n",
    "print([f1, acc, pre, rec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0650ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('models/gender.svc.bin', 'wb') as handle:\n",
    "#     pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('models/gender.vectorizer.bin', 'wb') as handle2:\n",
    "#     pickle.dump(vectorizer, handle2, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
