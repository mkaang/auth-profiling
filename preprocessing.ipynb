{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b310e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2328e731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users:  1050\n",
      "columns Index(['id_str', 'created_at', 'text', 'entities', 'retweeted', 'username',\n",
      "       'realname', 'gender', 'age', 'meslek', 'year', 'year_relative',\n",
      "       'age_normalized', 'gender_enc', 'age_group', 'age_group_norm',\n",
      "       'age_enc', 'age_enc_norm'],\n",
      "      dtype='object')\n",
      "train shape:  (1641471, 18) test shape (391212, 18)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('tweet_data.csv')\n",
    "print(\"number of users: \", df.username.unique().shape[0])\n",
    "print(\"columns\", df.columns)\n",
    "\n",
    "userlist_shuffled = pd.Series(df.username.unique()).sample(frac=1, random_state=26).reset_index(drop=True).tolist()\n",
    "\n",
    "mid = 4 * (len(userlist_shuffled) // 5)\n",
    "train = df[df.username.isin(userlist_shuffled[:mid])]\n",
    "test = df[df.username.isin(userlist_shuffled[mid:])]\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"train shape: \", train.shape, \"test shape\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca0bbfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-77f743f3919c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['year'] = train.created_at.apply(lambda x: x[:4])\n",
      "<ipython-input-3-77f743f3919c>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['year'] = test.created_at.apply(lambda x: x[:4])\n"
     ]
    }
   ],
   "source": [
    "# This cell just to be run for specifying years\n",
    "# train['year'] = train.created_at.apply(lambda x: x[:4])\n",
    "# test['year'] = test.created_at.apply(lambda x: x[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd870ffc",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f0a6924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Bozoklularr: Herkesin Ã¼niversitesi aÃ§Ä±lmÄ±ÅŸ, bir kafelerde,sokaklarda story atÄ±yorlar.\n",
      "Allah'Ä±m nasip et.ðŸ¤²\n",
      "RT @Bozoklularr: Herkesin Ã¼niversitesi aÃ§Ä±lmÄ±ÅŸ, bir kafelerde,sokaklarda story atÄ±yorlar.\n",
      "Allah'Ä±m nasip et.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "text = df.text[0]\n",
    "print(text) # with emoji\n",
    "\n",
    "print(remove_emoji(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a1fc4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(entities_df, text_df):\n",
    "    if isinstance(text_df, list): \n",
    "        prep_list = list()\n",
    "        for ent_df, txt_df in tqdm(zip(entities_df, text_df)):\n",
    "            prep_text=preprocess_ind(ent_df, txt_df)\n",
    "            prep_list.append(prep_text)\n",
    "        return prep_list\n",
    "    else:\n",
    "        return preprocess_ind(entities_df, text_df)\n",
    "    \n",
    "def preprocess_ind(entities_df, text_df):\n",
    "    ent_dict = eval(entities_df)\n",
    "    text = text_df\n",
    "    \n",
    "    # Extract entities directly from tweet entities\n",
    "    texts_to_extract = []\n",
    "    for key in ent_dict.keys():\n",
    "        if ent_dict[key]:\n",
    "            for ent_dict_2 in ent_dict[key]:\n",
    "                inds = ent_dict_2['indices']\n",
    "                texts_to_extract.append(text[inds[0]:inds[1]])\n",
    "    for text_to_extract in texts_to_extract:\n",
    "        text = text.replace(text_to_extract, '')\n",
    "        \n",
    "    # extract any words starting with # or @ or links starting with https\n",
    "    for tag in text.split(): \n",
    "        if tag.startswith(\"#\") or tag.startswith(\"@\") or tag.startswith(\"https://\"):\n",
    "            text=text.replace(tag, '')\n",
    "    \n",
    "    # extract newline and RT\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('RT : ', '')\n",
    "    \n",
    "    #demojidfy\n",
    "    text = remove_emoji(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a930fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Herkesin Ã¼niversitesi aÃ§Ä±lmÄ±ÅŸ, bir kafelerde,sokaklarda story atÄ±yorlar. Allah'Ä±m nasip et.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(df.entities[0], df.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1fa71c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1641471it [01:52, 14581.22it/s]\n",
      "C:\\Users\\MUSTAF~1\\AppData\\Local\\Temp/ipykernel_7648/3865866339.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"processed_text\"] = preprocess(train.entities.tolist(), train.text.tolist())\n",
      "391212it [00:27, 14036.97it/s]\n",
      "C:\\Users\\MUSTAF~1\\AppData\\Local\\Temp/ipykernel_7648/3865866339.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"processed_text\"] = preprocess(test.entities.tolist(), test.text.tolist())\n"
     ]
    }
   ],
   "source": [
    "train[\"processed_text\"] = preprocess(train.entities.tolist(), train.text.tolist())\n",
    "test[\"processed_text\"] = preprocess(test.entities.tolist(), test.text.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bb4a28",
   "metadata": {},
   "source": [
    "#### stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50a9cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "\n",
    "stemmer = TurkishStemmer()\n",
    "\n",
    "# stemmer=SnowballStemmer('porter')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def stem(text_df, stemmer):\n",
    "    stemmed_list = []\n",
    "    if isinstance(text_df, list): \n",
    "        for text in tqdm(text_df):\n",
    "            stemmed_list.append(stem_ind(text, stemmer))\n",
    "        return stemmed_list\n",
    "    else:\n",
    "        return stem_ind(text_df, stemmer)\n",
    "        \n",
    "def stem_ind(text_df, stemmer):\n",
    "    # Stem each word\n",
    "    stemmed =\"\"\n",
    "    #import pdb; pdb.set_trace()\n",
    "    worded = tokenizer.tokenize(text_df)\n",
    "    for word in worded:\n",
    "        stemmed = stemmed + stemmer.stem(word) + ' '\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb8f5202",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1641471/1641471 [05:15<00:00, 5206.01it/s] \n",
      "C:\\Users\\MUSTAF~1\\AppData\\Local\\Temp/ipykernel_7648/3266590763.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"stemmed\"] = stem(train[\"processed_text\"].tolist(), stemmer)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391212/391212 [01:12<00:00, 5373.49it/s] \n",
      "C:\\Users\\MUSTAF~1\\AppData\\Local\\Temp/ipykernel_7648/3266590763.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"stemmed\"] = stem(test[\"processed_text\"].tolist(), stemmer)\n"
     ]
    }
   ],
   "source": [
    "train[\"stemmed\"] = stem(train[\"processed_text\"].tolist(), stemmer)\n",
    "test[\"stemmed\"] = stem(test[\"processed_text\"].tolist(), stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75bead79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"tweets_prep_train.csv\", index=False)\n",
    "test.to_csv(\"tweets_prep_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbe1d0c",
   "metadata": {},
   "source": [
    "#### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69912e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary will be deleted\n",
    "import pandas as pd\n",
    "train = pd.read_csv(\"data/tweets_prep_train.csv\")\n",
    "test = pd.read_csv(\"data/tweets_prep_test.csv\")\n",
    "\n",
    "train = train[train.processed_text.notna() & train.stemmed.notna()]\n",
    "test = test[test.processed_text.notna() & test.stemmed.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b6e2f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary will be deleted\n",
    "most_commons = list()\n",
    "with open(f\"data/age_specific_words.txt\", encoding=\"utf-8\" ,mode=\"r\") as file:\n",
    "    for line in file.readlines():\n",
    "        most_commons.append(line.replace(\"\\n\", \"\"))\n",
    "        \n",
    "train_idxs = train.processed_text.apply(lambda x: any([(most_common in x) for most_common in most_commons]))\n",
    "train = train[train_idxs]\n",
    "\n",
    "test_idxs = test.processed_text.apply(lambda x: any([(most_common in x) for most_common in most_commons]))\n",
    "test = test[test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "556930e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_tweets(df, column:str, interval:str = None):\n",
    "    \"\"\"\n",
    "    concatenates tweets for each user\n",
    "    returns dataframe including usernames and corresponding concatenated tweets\n",
    "    \"\"\"\n",
    "    map_user = list()\n",
    "    map_tweets = list()\n",
    "    map_inter = list()\n",
    "    \n",
    "    if interval:\n",
    "        \n",
    "        unique_username_inter = pd.unique([(username, inter) for username, inter in zip(df[\"username\"], df[interval])])\n",
    "\n",
    "        for (username, inter) in tqdm(unique_username_inter):\n",
    "            \n",
    "            map_user.append(username)\n",
    "            map_inter.append(inter)\n",
    "\n",
    "            tweets_concat = \" \".join(df[(df.username == username) & (df[interval] == inter)][column].values)\n",
    "            map_tweets.append(tweets_concat)\n",
    "            \n",
    "        return pd.DataFrame.from_dict({\"username\":map_user, f\"tw_concat_{column}\":map_tweets, interval:map_inter})\n",
    "\n",
    "    else:\n",
    "\n",
    "        for username in tqdm(df[\"username\"].unique()):\n",
    "            \n",
    "            map_user.append(username)\n",
    "\n",
    "            tweets_concat = \" \".join(df[df[\"username\"] == username][column].values)\n",
    "            map_tweets.append(tweets_concat)\n",
    "            \n",
    "        return pd.DataFrame.from_dict({\"username\":map_user, f\"tw_concat_{column}\":map_tweets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1f1c8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 839/839 [01:10<00:00, 11.92it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 210/210 [00:36<00:00,  5.76it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 839/839 [01:16<00:00, 10.93it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 210/210 [00:09<00:00, 22.20it/s]\n"
     ]
    }
   ],
   "source": [
    "map_user_tweet_tr_stem = concat_tweets(train, 'stemmed')\n",
    "map_user_tweet_ts_stem = concat_tweets(test, 'stemmed')\n",
    "\n",
    "map_user_tweet_tr_process = concat_tweets(train, 'processed_text')\n",
    "map_user_tweet_ts_process = concat_tweets(test, 'processed_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05621480",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_user_tweet_tr = map_user_tweet_tr_stem.merge(map_user_tweet_tr_process, on=[\"username\"])\n",
    "map_user_tweet_ts = map_user_tweet_ts_stem.merge(map_user_tweet_ts_process, on=[\"username\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21fa52ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tr = map_user_tweet_tr.merge(train[['username',\n",
    "                   'realname','meslek', 'age_group',\n",
    "                   'age_enc', 'gender', 'gender_enc']].drop_duplicates(), how=\"left\", left_on=[\"username\"], right_on=[\"username\"])\n",
    "\n",
    "merged_ts = map_user_tweet_ts.merge(test[['username',\n",
    "                   'realname','meslek', 'age_group',\n",
    "                   'age_enc', 'gender', 'gender_enc']].drop_duplicates(), how=\"left\", left_on=[\"username\"], right_on=[\"username\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5019daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tr.to_csv(\"user_tweets_train_filtby_age.csv\", index=False)\n",
    "merged_ts.to_csv(\"user_tweets_test_filtby_age.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d4dece",
   "metadata": {},
   "source": [
    "### Filter only current age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0eafef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we just interested in the tweets for the current age group, filter only the current age group tweets\n",
    "# merged_tr = pd.read_csv(\"data/user_tweets_train_norm.csv\")\n",
    "# merged_ts = pd.read_csv(\"data/user_tweets_test_norm.csv\")\n",
    "\n",
    "merged_tr.to_csv(\"user_tweets_train.csv\", index=False)\n",
    "merged_ts.to_csv(\"user_tweets_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec7054",
   "metadata": {},
   "source": [
    "### Prepare Data for BERTTurk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd944c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_user_tweet_tr = concat_tweets(train, 'processed_text')\n",
    "# map_user_tweet_ts = concat_tweets(test, 'processed_text')\n",
    "\n",
    "# merged_tr = map_user_tweet_tr.merge(train[['username',\n",
    "#                    'realname','meslek', 'age_group',\n",
    "#                    'age_enc', 'gender', 'gender_enc']].drop_duplicates(), how=\"left\", left_on=\"username\", right_on=\"username\")\n",
    "\n",
    "# merged_ts = map_user_tweet_ts.merge(test[['username',\n",
    "#                    'realname','meslek', 'age_group',\n",
    "#                    'age_enc', 'gender', 'gender_enc']].drop_duplicates(), how=\"left\", left_on=\"username\", right_on=\"username\")\n",
    "\n",
    "# def process_data(df, label):\n",
    "#     column_titles = ['sentence1', 'label']\n",
    "#     df = df.rename(columns={'tw_concat':'sentence1', label:'label'})\n",
    "#     df = df[column_titles]\n",
    "#     df = df.reindex(columns=column_titles)\n",
    "#     return df\n",
    "\n",
    "# process_data(merged_tr, 'gender_enc').to_csv('train_gender.csv',index=False)\n",
    "# process_data(merged_ts, 'gender_enc').to_csv('test_gender.csv',index=False)\n",
    "\n",
    "# process_data(merged_tr, 'age_enc').to_csv('train_age.csv',index=False)\n",
    "# process_data(merged_ts, 'age_enc').to_csv('test_age.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
